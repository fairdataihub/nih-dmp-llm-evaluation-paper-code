{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "# Project root\n",
    "\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(20):\n",
    "        if (cur / \"inputs\").exists() and (cur / \"outputs\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT_DIR = find_project_root(Path.cwd())\n",
    "\n",
    "\n",
    "# Dataset location\n",
    "DATASET_DIR = ROOT_DIR / \"inputs\" / \"dataset\"\n",
    "INPUT_BRANCH = \"primary\"  \n",
    "\n",
    "\n",
    "# Automated-evaluation inputs\n",
    "\n",
    "AUTO_IN_DIR = DATASET_DIR / INPUT_BRANCH / \"automated-evaluation\"\n",
    "HUMAN_XLSX = AUTO_IN_DIR / \"Human.xlsx\"\n",
    "GPT_DIR    = AUTO_IN_DIR / \"gpt-DMPs\"\n",
    "LLAMA_DIR  = AUTO_IN_DIR / \"llama-DMPs\"\n",
    "\n",
    "\n",
    "# Automated-evaluation outputs\n",
    "AUTO_OUT_DIR = ROOT_DIR / \"outputs\" / \"automated-evaluation\"\n",
    "AUTO_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR = AUTO_OUT_DIR\n",
    "\n",
    "\n",
    "# Output filenames used later\n",
    "# Step 5 outputs\n",
    "GPT_OUT_CSV   = OUT_DIR / \"filtered_Gpt.csv\"\n",
    "LLAMA_OUT_CSV = OUT_DIR / \"filtered_Llama.csv\"\n",
    "# Step 6 outputs\n",
    "GPT_MERGED   = OUT_DIR / \"merged_output_Gpt.csv\"\n",
    "LLAMA_MERGED = OUT_DIR / \"merged_output_Llama.csv\"\n",
    "# Step 7 outputs\n",
    "GPT_CLEANED   = OUT_DIR / \"merged_output_Gpt_cleaned.csv\"\n",
    "LLAMA_CLEANED = OUT_DIR / \"merged_output_Llama_cleaned.csv\"\n",
    "# Step 8 outputs (collapsed elements)\n",
    "OUT_FOLDER = {\n",
    "    \"Gpt\":   OUT_DIR / \"dmp_similarity_summary_Gpt.csv\",\n",
    "    \"Llama\": OUT_DIR / \"dmp_similarity_summary_Llama.csv\",\n",
    "}\n",
    "OUT_RAW = {\n",
    "    \"Gpt\":   OUT_DIR / \"element_similarity_raw_Gpt.csv\",\n",
    "    \"Llama\": OUT_DIR / \"element_similarity_raw_Llama.csv\",\n",
    "}\n",
    "OUT_ELEMENT = {\n",
    "    \"Gpt\":   OUT_DIR / \"element_similarity_summary_Gpt.csv\",\n",
    "    \"Llama\": OUT_DIR / \"element_similarity_summary_Llama.csv\",\n",
    "}\n",
    "OUT_SUB_ELEMENT = {\n",
    "    \"Gpt\":   OUT_DIR / \"sub_element_similarity_summary_Gpt.csv\",\n",
    "    \"Llama\": OUT_DIR / \"sub_element_similarity_summary_Llama.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2 — Shared Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def safe_text(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(x)\n",
    "\n",
    "\n",
    "def norm_text(x) -> str:\n",
    "    s = safe_text(x).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def choose_title_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"title\", \"Title\", \"dmp_title\", \"DMP Title\", \"DMP_title\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"Could not find title column. Columns: {list(df.columns)}\")\n",
    "\n",
    "\n",
    "def sort_element_key(x: str):\n",
    "    s = str(x).strip().lower()\n",
    "    m = re.match(r\"^element_(\\d+)([a-z]?)$\", s)\n",
    "    if not m:\n",
    "        return (999, \"z\", s)\n",
    "    return (int(m.group(1)), m.group(2) or \"\", s)\n",
    "\n",
    "\n",
    "def rougeL_recall(ref: str, gen: str, scorer: rouge_scorer.RougeScorer) -> float:\n",
    "    ref = ref or \"\"\n",
    "    gen = gen or \"\"\n",
    "    if not ref.strip() and not gen.strip():\n",
    "        return 1.0\n",
    "    if not ref.strip() or not gen.strip():\n",
    "        return 0.0\n",
    "    return float(scorer.score(ref, gen)[\"rougeL\"].recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 — Markdown Parsing Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_title_for_match(name: str) -> str:\n",
    "    s = str(name).strip()\n",
    "    s = re.sub(r'[\\\\/*?:\"<>|]', \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalize_md_stem(stem: str) -> str:\n",
    "    s = (stem or \"\").strip().lower()\n",
    "    s = re.sub(r\"[-_\\s]?gpt[-_\\s]?[\\d\\.]+$\", \"\", s)\n",
    "    s = re.sub(r\"[-_\\s]?llama[-_\\s]?[\\d\\.]+$\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "\n",
    "def find_md_by_excel_title(search_dir: Path, excel_title: str) -> Path | None:\n",
    "    excel_norm = norm_text(clean_title_for_match(excel_title))\n",
    "    md_files = list(search_dir.rglob(\"*.md\"))\n",
    "\n",
    "    for p in md_files:\n",
    "        if normalize_md_stem(p.stem) == excel_norm:\n",
    "            return p\n",
    "\n",
    "    candidates = [p for p in md_files if excel_norm and excel_norm in normalize_md_stem(p.stem)]\n",
    "    if candidates:\n",
    "        candidates = sorted(candidates, key=lambda x: (len(x.stem), str(x)))\n",
    "        return candidates[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def is_title(line: str) -> bool:\n",
    "    s = (line or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if s.startswith(\"#\"):\n",
    "        return True\n",
    "    if re.match(r\"^\\s*\\d+[\\.\\)]?\\s*\\*\\*.+\\*\\*\\s*:?\\s*$\", s):\n",
    "        return True\n",
    "    if re.match(r\"^\\s*\\*\\*\\s*element\\s*\\d+\\s*:\\s*.+\\*\\*\\s*:?\\s*$\", s, flags=re.I):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def extract_titles_and_text(md_text: str, content_col: str) -> pd.DataFrame:\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", md_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    lines = cleaned.splitlines()\n",
    "\n",
    "    rows = []\n",
    "    current_title = None\n",
    "    buf = []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal current_title, buf, rows\n",
    "        if current_title is None:\n",
    "            return\n",
    "        text = \"\\n\".join(buf).strip()\n",
    "        if text:\n",
    "            rows.append({\"Element title\": current_title.strip(), content_col: text})\n",
    "\n",
    "    for line in lines:\n",
    "        if is_title(line):\n",
    "            flush()\n",
    "            current_title = line\n",
    "            buf = []\n",
    "        else:\n",
    "            buf.append(line)\n",
    "\n",
    "    flush()\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4 — Section Title → Element Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_section_title_to_element(section_title) -> str | None:\n",
    "    if section_title is None:\n",
    "        return None\n",
    "    try:\n",
    "        if pd.isna(section_title):\n",
    "            return None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    t = str(section_title).strip()\n",
    "    t_plain = re.sub(r\"^#+\\s*\", \"\", t).strip()\n",
    "    t_low = t_plain.lower()\n",
    "\n",
    "    if re.search(r\"\\belement\\s*2\\b\", t_low):\n",
    "        return \"element_2\"\n",
    "    if re.search(r\"\\belement\\s*3\\b\", t_low):\n",
    "        return \"element_3\"\n",
    "    if re.search(r\"\\belement\\s*6\\b\", t_low):\n",
    "        return \"element_6\"\n",
    "\n",
    "    if \"types and amount of scientific data\" in t_low:\n",
    "        return \"element_1a\"\n",
    "    if \"scientific data that will be preserved and shared\" in t_low:\n",
    "        return \"element_1b\"\n",
    "    if \"metadata, other relevant data, and associated documentation\" in t_low:\n",
    "        return \"element_1c\"\n",
    "\n",
    "    if \"repository where scientific data and metadata will be archived\" in t_low:\n",
    "        return \"element_4a\"\n",
    "    if \"how scientific data will be findable and identifiable\" in t_low:\n",
    "        return \"element_4b\"\n",
    "    if \"when and how long the scientific data will be made available\" in t_low:\n",
    "        return \"element_4c\"\n",
    "\n",
    "    if \"factors affecting subsequent access, distribution, or reuse\" in t_low:\n",
    "        return \"element_5a\"\n",
    "    if \"whether access to scientific data will be controlled\" in t_low:\n",
    "        return \"element_5b\"\n",
    "    if \"protections for privacy, rights, and confidentiality\" in t_low:\n",
    "        return \"element_5c\"\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_element_columns(df: pd.DataFrame) -> list:\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        cc = str(c).strip().lower()\n",
    "        if re.match(r\"^element_\\d+[a-z]?$\", cc):\n",
    "            cols.append(cc)\n",
    "    if not cols:\n",
    "        raise ValueError(\"No element_* columns found in Human.xlsx.\")\n",
    "\n",
    "    return sorted(cols, key=sort_element_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5 — Extract NIH Template Sections From Markdown (per title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Nahid\\nih-dmp-llm-evaluation-paper-code\\outputs\\automated-evaluation\\filtered_Gpt.csv | rows=312\n",
      "status\n",
      "ok    312\n",
      "Name: count, dtype: int64\n",
      "Saved: C:\\Users\\Nahid\\nih-dmp-llm-evaluation-paper-code\\outputs\\automated-evaluation\\filtered_Llama.csv | rows=312\n",
      "status\n",
      "ok    312\n",
      "Name: count, dtype: int64\n",
      "Done: Step 5\n"
     ]
    }
   ],
   "source": [
    "human_df = pd.read_excel(HUMAN_XLSX)\n",
    "title_col = choose_title_column(human_df)\n",
    "titles = human_df[title_col].dropna().astype(str).tolist()\n",
    "\n",
    "\n",
    "def process_model_folder(model_dir: Path, content_col: str, out_csv: Path) -> pd.DataFrame:\n",
    "    records = []\n",
    "\n",
    "    for title in titles:\n",
    "        md_path = find_md_by_excel_title(model_dir, title)\n",
    "\n",
    "        if md_path is None:\n",
    "            records.append(\n",
    "                {\n",
    "                    \"dmp_title\": title,\n",
    "                    \"md_path\": None,\n",
    "                    \"Element title\": None,\n",
    "                    content_col: None,\n",
    "                    \"status\": \"missing_md\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        md_text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        df_sec = extract_titles_and_text(md_text, content_col=content_col)\n",
    "\n",
    "        if df_sec.empty:\n",
    "            records.append(\n",
    "                {\n",
    "                    \"dmp_title\": title,\n",
    "                    \"md_path\": str(md_path),\n",
    "                    \"Element title\": None,\n",
    "                    content_col: None,\n",
    "                    \"status\": \"no_sections_found\",\n",
    "                }\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        for _, r in df_sec.iterrows():\n",
    "            records.append(\n",
    "                {\n",
    "                    \"dmp_title\": title,\n",
    "                    \"md_path\": str(md_path),\n",
    "                    \"Element title\": r[\"Element title\"],\n",
    "                    content_col: r[content_col],\n",
    "                    \"status\": \"ok\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    out_df = pd.DataFrame(records)\n",
    "    out_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_csv} | rows={len(out_df)}\")\n",
    "    print(out_df[\"status\"].value_counts(dropna=False))\n",
    "    return out_df\n",
    "\n",
    "\n",
    "process_model_folder(GPT_DIR, \"Generated_Gpt_content\", GPT_OUT_CSV)\n",
    "process_model_folder(LLAMA_DIR, \"Generated_Llama_content\", LLAMA_OUT_CSV)\n",
    "print(\"Done: Step 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6 — Merge NIH Reference With Model Output (sub-elements preserved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Nahid\\nih-dmp-llm-evaluation-paper-code\\outputs\\automated-evaluation\\merged_output_Gpt.csv | rows=312\n",
      "Saved: C:\\Users\\Nahid\\nih-dmp-llm-evaluation-paper-code\\outputs\\automated-evaluation\\merged_output_Llama.csv | rows=312\n",
      "Done: Step 6\n"
     ]
    }
   ],
   "source": [
    "ref = pd.read_excel(HUMAN_XLSX)\n",
    "title_col = choose_title_column(ref)\n",
    "ref = ref.rename(columns={title_col: \"title\"})\n",
    "ref.columns = [str(c).strip().lower() for c in ref.columns]\n",
    "\n",
    "ref[\"title_norm\"] = ref[\"title\"].apply(norm_text)\n",
    "element_cols = get_element_columns(ref)\n",
    "\n",
    "ref_long = ref[[\"title\", \"title_norm\"] + element_cols].fillna(\"\").melt(\n",
    "    id_vars=[\"title\", \"title_norm\"],\n",
    "    value_vars=element_cols,\n",
    "    var_name=\"Element number\",\n",
    "    value_name=\"NIH Value\",\n",
    ")\n",
    "ref_long[\"Element number\"] = ref_long[\"Element number\"].str.lower().str.strip()\n",
    "\n",
    "\n",
    "def load_model_filtered(path: Path, content_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"title_norm\"] = df[\"dmp_title\"].apply(norm_text)\n",
    "    df[\"Element title\"] = df[\"Element title\"].fillna(\"\").astype(str)\n",
    "\n",
    "    df[\"Element number\"] = df[\"Element title\"].apply(map_section_title_to_element)\n",
    "    df = df[df[\"Element number\"].notna()].copy()\n",
    "    df[\"Element number\"] = df[\"Element number\"].str.lower().str.strip()\n",
    "\n",
    "    df = df.rename(columns={content_col: \"Generated Content\"})\n",
    "    return df[[\"title_norm\", \"Element number\", \"Element title\", \"Generated Content\"]]\n",
    "\n",
    "\n",
    "def merge_and_save(sec_df: pd.DataFrame, out_path: Path):\n",
    "    merged = ref_long.merge(sec_df, on=[\"title_norm\", \"Element number\"], how=\"left\")\n",
    "    merged = merged[[\"title\", \"Element number\", \"NIH Value\", \"Element title\", \"Generated Content\"]]\n",
    "    merged.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_path} | rows={len(merged)}\")\n",
    "\n",
    "\n",
    "gpt_sec = load_model_filtered(GPT_OUT_CSV, \"Generated_Gpt_content\")\n",
    "llm_sec = load_model_filtered(LLAMA_OUT_CSV, \"Generated_Llama_content\")\n",
    "\n",
    "merge_and_save(gpt_sec, GPT_MERGED)\n",
    "merge_and_save(llm_sec, LLAMA_MERGED)\n",
    "print(\"Done: Step 6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7 — Collapse Sub-elements to Core Elements (1, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Nahid\\nih-dmp-llm-evaluation-paper-code\\outputs\\automated-evaluation\\merged_output_Gpt_cleaned.csv | rows=156\n",
      "Element counts: {'element_1': 26, 'element_2': 26, 'element_3': 26, 'element_4': 26, 'element_5': 26, 'element_6': 26}\n",
      "Saved: C:\\Users\\Nahid\\nih-dmp-llm-evaluation-paper-code\\outputs\\automated-evaluation\\merged_output_Llama_cleaned.csv | rows=156\n",
      "Element counts: {'element_1': 26, 'element_2': 26, 'element_3': 26, 'element_4': 26, 'element_5': 26, 'element_6': 26}\n",
      "Done: Step 7\n"
     ]
    }
   ],
   "source": [
    "INPUTS = {\"Gpt\": GPT_MERGED, \"Llama\": LLAMA_MERGED}\n",
    "OUTPUTS = {\"Gpt\": GPT_CLEANED, \"Llama\": LLAMA_CLEANED}\n",
    "\n",
    "group_map = {\n",
    "    \"element_1\": [\"element_1a\", \"element_1b\", \"element_1c\"],\n",
    "    \"element_4\": [\"element_4a\", \"element_4b\", \"element_4c\"],\n",
    "    \"element_5\": [\"element_5a\", \"element_5b\", \"element_5c\"],\n",
    "}\n",
    "\n",
    "\n",
    "def clean_text(x) -> str:\n",
    "    s = safe_text(x).strip()\n",
    "    s = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def combine_group_for_title(df_title: pd.DataFrame, new_element: str, group: list[str]) -> dict:\n",
    "    g = df_title[df_title[\"Element number\"].isin(group)].copy()\n",
    "    g[\"Element number\"] = pd.Categorical(g[\"Element number\"], categories=group, ordered=True)\n",
    "    g = g.sort_values(\"Element number\")\n",
    "\n",
    "    nih = \"\\n\\n\".join([clean_text(v) for v in g[\"NIH Value\"].tolist() if clean_text(v)])\n",
    "    gen = \"\\n\\n\".join([clean_text(v) for v in g[\"Generated Content\"].tolist() if clean_text(v)])\n",
    "\n",
    "    return {\"Element number\": new_element, \"NIH Value\": nih, \"Generated Content\": gen}\n",
    "\n",
    "\n",
    "for model_name, in_path in INPUTS.items():\n",
    "    if not in_path.exists():\n",
    "        print(f\"Missing input: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    required = {\"title\", \"Element number\", \"NIH Value\", \"Generated Content\"}\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{in_path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    df[\"Element number\"] = df[\"Element number\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    group_flat = [e for grp in group_map.values() for e in grp]\n",
    "    out_blocks = []\n",
    "\n",
    "    for title, df_title in df.groupby(\"title\", dropna=False):\n",
    "        df_title = df_title.copy()\n",
    "\n",
    "        keep_df = df_title[~df_title[\"Element number\"].isin(group_flat)].copy()\n",
    "        keep_df = keep_df[[\"Element number\", \"NIH Value\", \"Generated Content\"]]\n",
    "\n",
    "        merged_rows = []\n",
    "        for new_element, grp in group_map.items():\n",
    "            merged_rows.append(combine_group_for_title(df_title, new_element, grp))\n",
    "\n",
    "        final_title_df = pd.concat([keep_df, pd.DataFrame(merged_rows)], ignore_index=True)\n",
    "\n",
    "        for col in [\"NIH Value\", \"Generated Content\"]:\n",
    "            final_title_df[col] = final_title_df[col].apply(clean_text)\n",
    "\n",
    "        final_title_df = final_title_df.sort_values(\n",
    "            by=\"Element number\", key=lambda s: s.map(sort_element_key)\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        final_title_df.insert(0, \"title\", title)\n",
    "        out_blocks.append(final_title_df)\n",
    "\n",
    "    final_df = pd.concat(out_blocks, ignore_index=True)\n",
    "    out_path = OUTPUTS[model_name]\n",
    "    final_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\"Saved: {out_path} | rows={len(final_df)}\")\n",
    "    print(\"Element counts:\", final_df[\"Element number\"].value_counts().to_dict())\n",
    "\n",
    "print(\"Done: Step 7\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8 — Similarity Scoring (collapsed elements) + Sub-element Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1106.70it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Step 8 outputs for Gpt:\n",
      "  C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/dmp_similarity_summary_Gpt.csv\n",
      "  C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/element_similarity_raw_Gpt.csv\n",
      "  C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/element_similarity_summary_Gpt.csv\n",
      "Saved Step 8 outputs for Llama:\n",
      "  C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/dmp_similarity_summary_Llama.csv\n",
      "  C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/element_similarity_raw_Llama.csv\n",
      "  C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/element_similarity_summary_Llama.csv\n",
      "Done: Step 8a\n",
      "Saved: C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/sub_element_similarity_summary_Gpt.csv\n",
      "Saved: C:/Users/Nahid/nih-dmp-llm-evaluation-paper-code/outputs/automated-evaluation/sub_element_similarity_summary_Llama.csv\n",
      "Done: Step 8b\n"
     ]
    }
   ],
   "source": [
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "\n",
    "def compute_similarity_tables(\n",
    "    in_path: Path,\n",
    "    model_name: str,\n",
    "    write_folder_summary: bool,\n",
    "    folder_summary_path: Path | None,\n",
    "    raw_path: Path | None,\n",
    "    element_summary_path: Path | None,\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    required = {\"title\", \"Element number\", \"NIH Value\", \"Generated Content\"}\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{in_path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    df[\"title\"] = df[\"title\"].apply(safe_text)\n",
    "    df[\"Element number\"] = df[\"Element number\"].apply(safe_text).str.strip().str.lower()\n",
    "    df[\"NIH Value\"] = df[\"NIH Value\"].apply(safe_text)\n",
    "    df[\"Generated Content\"] = df[\"Generated Content\"].apply(safe_text)\n",
    "\n",
    "    nih_texts = df[\"NIH Value\"].tolist()\n",
    "    gen_texts = df[\"Generated Content\"].tolist()\n",
    "\n",
    "    emb_nih = sbert.encode(nih_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    emb_gen = sbert.encode(gen_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    sbert_sims = (emb_nih * emb_gen).sum(dim=1).cpu().numpy().astype(float)\n",
    "\n",
    "    rouge_recalls = [rougeL_recall(r, g, rouge) for r, g in zip(nih_texts, gen_texts)]\n",
    "\n",
    "    raw = pd.DataFrame(\n",
    "        {\n",
    "            \"Model\": model_name,\n",
    "            \"DMP Title\": df[\"title\"],\n",
    "            \"Element number\": df[\"Element number\"],\n",
    "            \"SBERT_Similarity\": sbert_sims,\n",
    "            \"ROUGE_L_Recall\": rouge_recalls,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if raw_path is not None:\n",
    "        raw.to_csv(raw_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    if write_folder_summary and folder_summary_path is not None:\n",
    "        folder_summary = (\n",
    "            raw.groupby([\"Model\", \"DMP Title\"], as_index=False)[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]]\n",
    "            .mean()\n",
    "            .rename(columns={\"DMP Title\": \"Folder\"})\n",
    "        )\n",
    "        folder_summary.to_csv(folder_summary_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    if element_summary_path is not None:\n",
    "        element_summary = (\n",
    "            raw.groupby([\"Model\", \"Element number\"], as_index=False)[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]]\n",
    "            .mean()\n",
    "        )\n",
    "        element_summary.to_csv(element_summary_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return raw\n",
    "\n",
    "\n",
    "# Step 8a: collapsed elements (from cleaned files)\n",
    "clean_inputs = {\"Gpt\": GPT_CLEANED, \"Llama\": LLAMA_CLEANED}\n",
    "\n",
    "for model_name, in_path in clean_inputs.items():\n",
    "    if not in_path.exists():\n",
    "        print(f\"Missing input: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    compute_similarity_tables(\n",
    "        in_path=in_path,\n",
    "        model_name=model_name,\n",
    "        write_folder_summary=True,\n",
    "        folder_summary_path=OUT_FOLDER[model_name],\n",
    "        raw_path=OUT_RAW[model_name],\n",
    "        element_summary_path=OUT_ELEMENT[model_name],\n",
    "    )\n",
    "\n",
    "    print(f\"Saved Step 8 outputs for {model_name}:\")\n",
    "    print(f\"  {OUT_FOLDER[model_name].as_posix()}\")\n",
    "    print(f\"  {OUT_RAW[model_name].as_posix()}\")\n",
    "    print(f\"  {OUT_ELEMENT[model_name].as_posix()}\")\n",
    "\n",
    "print(\"Done: Step 8a\")\n",
    "\n",
    "\n",
    "# Step 8b: sub-element summary (from non-cleaned merged files)\n",
    "sub_inputs = {\"Gpt\": GPT_MERGED, \"Llama\": LLAMA_MERGED}\n",
    "\n",
    "for model_name, in_path in sub_inputs.items():\n",
    "    if not in_path.exists():\n",
    "        print(f\"Missing input: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    raw_sub = compute_similarity_tables(\n",
    "        in_path=in_path,\n",
    "        model_name=model_name,\n",
    "        write_folder_summary=False,\n",
    "        folder_summary_path=None,\n",
    "        raw_path=None,\n",
    "        element_summary_path=None,\n",
    "    )\n",
    "\n",
    "    sub_element_summary = (\n",
    "        raw_sub.groupby([\"Model\", \"Element number\"], as_index=False)[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]]\n",
    "        .mean()\n",
    "        .sort_values(by=\"Element number\", key=lambda s: s.map(sort_element_key))\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    sub_element_summary.to_csv(OUT_SUB_ELEMENT[model_name], index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {OUT_SUB_ELEMENT[model_name].as_posix()}\")\n",
    "\n",
    "print(\"Done: Step 8b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
