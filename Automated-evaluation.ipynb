{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated-evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: derivative\\filtered_Gpt.csv | rows=312\n",
      "status\n",
      "ok    312\n",
      "Name: count, dtype: int64\n",
      "Saved: derivative\\filtered_Llama.csv | rows=312\n",
      "status\n",
      "ok    312\n",
      "Name: count, dtype: int64\n",
      "Done: Step 5 (UPDATED)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PRIMARY_DIR = Path(\"primary\")\n",
    "HUMAN_XLSX  = PRIMARY_DIR / \"Human.xlsx\"\n",
    "GPT_DIR     = PRIMARY_DIR / \"gpt-DMPs\"\n",
    "LLAMA_DIR   = PRIMARY_DIR / \"llama-DMPs\"\n",
    "\n",
    "DERIV_DIR = Path(\"derivative\")\n",
    "DERIV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GPT_OUT_CSV   = DERIV_DIR / \"filtered_Gpt.csv\"\n",
    "LLAMA_OUT_CSV = DERIV_DIR / \"filtered_Llama.csv\"\n",
    "\n",
    "def norm_text(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def clean_title_for_match(name: str) -> str:\n",
    "    s = str(name).strip()\n",
    "    s = re.sub(r'[\\\\/*?:\"<>|]', \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_md_stem(stem: str) -> str:\n",
    "    s = (stem or \"\").strip().lower()\n",
    "    s = re.sub(r\"[-_\\s]?gpt[-_\\s]?[\\d\\.]+$\", \"\", s)\n",
    "    s = re.sub(r\"[-_\\s]?llama[-_\\s]?[\\d\\.]+$\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def find_md_by_excel_title(search_dir: Path, excel_title: str) -> Path | None:\n",
    "    excel_norm = norm_text(clean_title_for_match(excel_title))\n",
    "    md_files = list(search_dir.rglob(\"*.md\"))\n",
    "\n",
    "    for p in md_files:\n",
    "        if normalize_md_stem(p.stem) == excel_norm:\n",
    "            return p\n",
    "\n",
    "    candidates = [p for p in md_files if excel_norm and excel_norm in normalize_md_stem(p.stem)]\n",
    "    if candidates:\n",
    "        candidates = sorted(candidates, key=lambda x: (len(x.stem), str(x)))\n",
    "        return candidates[0]\n",
    "    return None\n",
    "\n",
    "def is_title(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Title detection for NIH template-style outputs:\n",
    "    - Markdown headers: #, ##, ...\n",
    "    - Numbered bold prompts: 1. **...**\n",
    "    - Bold element headers: **Element 2: ...**\n",
    "    \"\"\"\n",
    "    s = (line or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "\n",
    "    if s.startswith(\"#\"):\n",
    "        return True\n",
    "\n",
    "    # 1. **Types...**\n",
    "    if re.match(r\"^\\s*\\d+[\\.\\)]?\\s*\\*\\*.+\\*\\*\\s*:?\\s*$\", s):\n",
    "        return True\n",
    "\n",
    "    # **Element 2: Related Tools...**\n",
    "    if re.match(r\"^\\s*\\*\\*\\s*element\\s*\\d+\\s*:\\s*.+\\*\\*\\s*:?\\s*$\", s, flags=re.I):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def extract_titles_and_text(md_text: str, content_col: str) -> pd.DataFrame:\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", md_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    lines = cleaned.splitlines()\n",
    "\n",
    "    rows = []\n",
    "    current_title = None\n",
    "    buf = []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal current_title, buf, rows\n",
    "        if current_title is None:\n",
    "            return\n",
    "        text = \"\\n\".join(buf).strip()\n",
    "        if text:\n",
    "            rows.append({\"Element title\": current_title.strip(), content_col: text})\n",
    "\n",
    "    for line in lines:\n",
    "        if is_title(line):\n",
    "            flush()\n",
    "            current_title = line\n",
    "            buf = []\n",
    "        else:\n",
    "            buf.append(line)\n",
    "\n",
    "    flush()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def choose_title_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"title\", \"Title\", \"dmp_title\", \"DMP Title\", \"DMP_title\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"Could not find title column. Columns: {list(df.columns)}\")\n",
    "\n",
    "human_df = pd.read_excel(HUMAN_XLSX)\n",
    "title_col = choose_title_column(human_df)\n",
    "titles = human_df[title_col].dropna().astype(str).tolist()\n",
    "\n",
    "def process_model_folder(model_dir: Path, content_col: str, out_csv: Path) -> pd.DataFrame:\n",
    "    records = []\n",
    "\n",
    "    for title in titles:\n",
    "        md_path = find_md_by_excel_title(model_dir, title)\n",
    "\n",
    "        if md_path is None:\n",
    "            records.append({\n",
    "                \"dmp_title\": title,\n",
    "                \"md_path\": None,\n",
    "                \"Element title\": None,\n",
    "                content_col: None,\n",
    "                \"status\": \"missing_md\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        md_text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        df_sec = extract_titles_and_text(md_text, content_col=content_col)\n",
    "\n",
    "        if df_sec.empty:\n",
    "            records.append({\n",
    "                \"dmp_title\": title,\n",
    "                \"md_path\": str(md_path),\n",
    "                \"Element title\": None,\n",
    "                content_col: None,\n",
    "                \"status\": \"no_sections_found\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for _, r in df_sec.iterrows():\n",
    "            records.append({\n",
    "                \"dmp_title\": title,\n",
    "                \"md_path\": str(md_path),\n",
    "                \"Element title\": r[\"Element title\"],\n",
    "                content_col: r[content_col],\n",
    "                \"status\": \"ok\"\n",
    "            })\n",
    "\n",
    "    out_df = pd.DataFrame(records)\n",
    "    out_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_csv} | rows={len(out_df)}\")\n",
    "    print(out_df[\"status\"].value_counts(dropna=False))\n",
    "    return out_df\n",
    "\n",
    "process_model_folder(GPT_DIR,   \"Generated_Gpt_content\",   GPT_OUT_CSV)\n",
    "process_model_folder(LLAMA_DIR, \"Generated_Llama_content\", LLAMA_OUT_CSV)\n",
    "\n",
    "print(\"Done: Step 5 (UPDATED)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: derivative\\merged_output_Gpt.csv | rows=312\n",
      "Saved: derivative\\merged_output_Llama.csv | rows=312\n",
      "Done: Step 6 (UPDATED)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "PRIMARY_DIR = Path(\"primary\")\n",
    "HUMAN_XLSX  = PRIMARY_DIR / \"Human.xlsx\"\n",
    "\n",
    "DERIV_DIR = Path(\"derivative\")\n",
    "GPT_FILTERED   = DERIV_DIR / \"filtered_Gpt.csv\"\n",
    "LLAMA_FILTERED = DERIV_DIR / \"filtered_Llama.csv\"\n",
    "\n",
    "GPT_MERGED   = DERIV_DIR / \"merged_output_Gpt.csv\"\n",
    "LLAMA_MERGED = DERIV_DIR / \"merged_output_Llama.csv\"\n",
    "\n",
    "def norm_text(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def choose_title_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"title\", \"Title\", \"dmp_title\", \"DMP Title\", \"DMP_title\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"Could not find title column. Columns: {list(df.columns)}\")\n",
    "\n",
    "def get_element_columns(df: pd.DataFrame) -> list:\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        cc = str(c).strip().lower()\n",
    "        if re.match(r\"^element_\\d+[a-z]?$\", cc):\n",
    "            cols.append(cc)\n",
    "    if not cols:\n",
    "        raise ValueError(\"No element_* columns found in Human.xlsx.\")\n",
    "\n",
    "    def sort_key(c):\n",
    "        m = re.match(r\"^element_(\\d+)([a-z]?)$\", c)\n",
    "        return (int(m.group(1)), m.group(2) or \"\")\n",
    "    return sorted(cols, key=sort_key)\n",
    "\n",
    "def map_section_title_to_element(section_title) -> str | None:\n",
    "    if section_title is None:\n",
    "        return None\n",
    "    try:\n",
    "        if pd.isna(section_title):\n",
    "            return None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    t = str(section_title).strip()\n",
    "    t_plain = re.sub(r\"^#+\\s*\", \"\", t).strip()\n",
    "    t_low = t_plain.lower()\n",
    "\n",
    "    # Element headers\n",
    "    if re.search(r\"\\belement\\s*2\\b\", t_low): return \"element_2\"\n",
    "    if re.search(r\"\\belement\\s*3\\b\", t_low): return \"element_3\"\n",
    "    if re.search(r\"\\belement\\s*6\\b\", t_low): return \"element_6\"\n",
    "\n",
    "    # Element 1 sub-questions (1a/1b/1c)\n",
    "    if \"types and amount of scientific data\" in t_low: return \"element_1a\"\n",
    "    if \"scientific data that will be preserved and shared\" in t_low: return \"element_1b\"\n",
    "    if \"metadata, other relevant data, and associated documentation\" in t_low: return \"element_1c\"\n",
    "\n",
    "    # Element 4 sub-questions (4a/4b/4c)\n",
    "    if \"repository where scientific data and metadata will be archived\" in t_low: return \"element_4a\"\n",
    "    if \"how scientific data will be findable and identifiable\" in t_low: return \"element_4b\"\n",
    "    if \"when and how long the scientific data will be made available\" in t_low: return \"element_4c\"\n",
    "\n",
    "    # Element 5 sub-questions (5a/5b/5c)\n",
    "    if \"factors affecting subsequent access, distribution, or reuse\" in t_low: return \"element_5a\"\n",
    "    if \"whether access to scientific data will be controlled\" in t_low: return \"element_5b\"\n",
    "    if \"protections for privacy, rights, and confidentiality\" in t_low: return \"element_5c\"\n",
    "\n",
    "    return None\n",
    "\n",
    "# NIH reference\n",
    "ref = pd.read_excel(HUMAN_XLSX)\n",
    "title_col = choose_title_column(ref)\n",
    "ref = ref.rename(columns={title_col: \"title\"})\n",
    "ref.columns = [str(c).strip().lower() for c in ref.columns]\n",
    "\n",
    "ref[\"title_norm\"] = ref[\"title\"].apply(norm_text)\n",
    "element_cols = get_element_columns(ref)\n",
    "\n",
    "ref_long = ref[[\"title\", \"title_norm\"] + element_cols].fillna(\"\").melt(\n",
    "    id_vars=[\"title\", \"title_norm\"],\n",
    "    value_vars=element_cols,\n",
    "    var_name=\"Element number\",\n",
    "    value_name=\"NIH Value\",\n",
    ")\n",
    "ref_long[\"Element number\"] = ref_long[\"Element number\"].str.lower().str.strip()\n",
    "\n",
    "def load_model_filtered(path: Path, content_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"title_norm\"] = df[\"dmp_title\"].apply(norm_text)\n",
    "    df[\"Element title\"] = df[\"Element title\"].fillna(\"\").astype(str)\n",
    "\n",
    "    df[\"Element number\"] = df[\"Element title\"].apply(map_section_title_to_element)\n",
    "    df = df[df[\"Element number\"].notna()].copy()\n",
    "    df[\"Element number\"] = df[\"Element number\"].str.lower().str.strip()\n",
    "\n",
    "    df = df.rename(columns={content_col: \"Generated Content\"})\n",
    "    return df[[\"title_norm\", \"Element number\", \"Element title\", \"Generated Content\"]]\n",
    "\n",
    "gpt_sec = load_model_filtered(GPT_FILTERED, \"Generated_Gpt_content\")\n",
    "llm_sec = load_model_filtered(LLAMA_FILTERED, \"Generated_Llama_content\")\n",
    "\n",
    "def merge_and_save(sec_df: pd.DataFrame, out_path: Path):\n",
    "    merged = ref_long.merge(sec_df, on=[\"title_norm\", \"Element number\"], how=\"left\")\n",
    "    merged = merged[[\"title\", \"Element number\", \"NIH Value\", \"Element title\", \"Generated Content\"]]\n",
    "    merged.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_path} | rows={len(merged)}\")\n",
    "\n",
    "merge_and_save(gpt_sec, GPT_MERGED)\n",
    "merge_and_save(llm_sec, LLAMA_MERGED)\n",
    "\n",
    "print(\"Done: Step 6 (UPDATED)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: derivative\\merged_output_Gpt_cleaned.csv | rows=156\n",
      "Element counts: {'element_1': 26, 'element_2': 26, 'element_3': 26, 'element_4': 26, 'element_5': 26, 'element_6': 26}\n",
      " Saved: derivative\\merged_output_Llama_cleaned.csv | rows=156\n",
      "Element counts: {'element_1': 26, 'element_2': 26, 'element_3': 26, 'element_4': 26, 'element_5': 26, 'element_6': 26}\n",
      "Done: Step 7\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DERIV_DIR = Path(\"derivative\")\n",
    "DERIV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INPUTS = {\n",
    "    \"Gpt\": DERIV_DIR / \"merged_output_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"merged_output_Llama.csv\",\n",
    "}\n",
    "OUTPUTS = {\n",
    "    \"Gpt\": DERIV_DIR / \"merged_output_Gpt_cleaned.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"merged_output_Llama_cleaned.csv\",\n",
    "}\n",
    "\n",
    "# Collapse these sub-elements into one per title\n",
    "group_map = {\n",
    "    \"element_1\": [\"element_1a\", \"element_1b\", \"element_1c\"],\n",
    "    \"element_4\": [\"element_4a\", \"element_4b\", \"element_4c\"],\n",
    "    \"element_5\": [\"element_5a\", \"element_5b\", \"element_5c\"],\n",
    "}\n",
    "\n",
    "def clean_text(x) -> str:\n",
    "    \"\"\"Trim + normalize blank lines; safe for NaN.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(x).strip()\n",
    "    s = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", s)  # collapse multi-blank lines\n",
    "    return s\n",
    "\n",
    "def sort_element_key(x: str):\n",
    "    \"\"\"element_1, element_2, ... element_6 ordering.\"\"\"\n",
    "    s = str(x).strip().lower()\n",
    "    m = re.match(r\"^element_(\\d+)([a-z]?)$\", s)\n",
    "    if not m:\n",
    "        return (999, \"z\", s)\n",
    "    return (int(m.group(1)), m.group(2) or \"\", s)\n",
    "\n",
    "def combine_group_for_title(df_title: pd.DataFrame, new_element: str, group: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Combine NIH Value and Generated Content for the given group, preserving group order.\n",
    "    \"\"\"\n",
    "    g = df_title[df_title[\"Element number\"].isin(group)].copy()\n",
    "    g[\"Element number\"] = pd.Categorical(g[\"Element number\"], categories=group, ordered=True)\n",
    "    g = g.sort_values(\"Element number\")\n",
    "\n",
    "    nih = \"\\n\\n\".join([clean_text(v) for v in g[\"NIH Value\"].tolist() if clean_text(v)])\n",
    "    gen = \"\\n\\n\".join([clean_text(v) for v in g[\"Generated Content\"].tolist() if clean_text(v)])\n",
    "\n",
    "    return {\n",
    "        \"Element number\": new_element,\n",
    "        \"NIH Value\": nih,\n",
    "        \"Generated Content\": gen,\n",
    "    }\n",
    "\n",
    "for model_name, in_path in INPUTS.items():\n",
    "    if not in_path.exists():\n",
    "        print(f\"⚠️ Missing input: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    required = {\"title\", \"Element number\", \"NIH Value\", \"Generated Content\"}\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{in_path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    # normalize\n",
    "    df[\"Element number\"] = df[\"Element number\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    group_flat = [e for grp in group_map.values() for e in grp]\n",
    "    out_blocks = []\n",
    "\n",
    "    # IMPORTANT: combine within each title\n",
    "    for title, df_title in df.groupby(\"title\", dropna=False):\n",
    "        df_title = df_title.copy()\n",
    "\n",
    "        keep_df = df_title[~df_title[\"Element number\"].isin(group_flat)].copy()\n",
    "        keep_df = keep_df[[\"Element number\", \"NIH Value\", \"Generated Content\"]]\n",
    "\n",
    "        merged_rows = []\n",
    "        for new_element, grp in group_map.items():\n",
    "            merged_rows.append(combine_group_for_title(df_title, new_element, grp))\n",
    "\n",
    "        final_title_df = pd.concat(\n",
    "            [keep_df, pd.DataFrame(merged_rows)],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        # clean text\n",
    "        for col in [\"NIH Value\", \"Generated Content\"]:\n",
    "            final_title_df[col] = final_title_df[col].apply(clean_text)\n",
    "\n",
    "        # sort + attach title\n",
    "        final_title_df = final_title_df.sort_values(\n",
    "            by=\"Element number\",\n",
    "            key=lambda s: s.map(sort_element_key)\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        final_title_df.insert(0, \"title\", title)\n",
    "        out_blocks.append(final_title_df)\n",
    "\n",
    "    final_df = pd.concat(out_blocks, ignore_index=True)\n",
    "\n",
    "    out_path = OUTPUTS[model_name]\n",
    "    final_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # quick sanity check\n",
    "    print(f\" Saved: {out_path} | rows={len(final_df)}\")\n",
    "    print(\"Element counts:\", final_df[\"Element number\"].value_counts().to_dict())\n",
    "\n",
    "print(\"Done: Step 7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1562.94it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved Step 8 outputs for Gpt:\n",
      "   - derivative/folder_similarity_summary_Gpt.csv\n",
      "   - derivative/element_similarity_raw_Gpt.csv\n",
      "   - derivative/element_similarity_summary_Gpt.csv\n",
      " Saved Step 8 outputs for Llama:\n",
      "   - derivative/folder_similarity_summary_Llama.csv\n",
      "   - derivative/element_similarity_raw_Llama.csv\n",
      "   - derivative/element_similarity_summary_Llama.csv\n",
      "Done: Step 8\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "DERIV_DIR = Path(\"derivative\")\n",
    "\n",
    "INPUTS = {\n",
    "    \"Gpt\":   DERIV_DIR / \"merged_output_Gpt_cleaned.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"merged_output_Llama_cleaned.csv\",\n",
    "}\n",
    "\n",
    "OUT_FOLDER = {\n",
    "    \"Gpt\":   DERIV_DIR / \"folder_similarity_summary_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"folder_similarity_summary_Llama.csv\",\n",
    "}\n",
    "OUT_RAW = {\n",
    "    \"Gpt\":   DERIV_DIR / \"element_similarity_raw_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"element_similarity_raw_Llama.csv\",\n",
    "}\n",
    "OUT_ELEMENT = {\n",
    "    \"Gpt\":   DERIV_DIR / \"element_similarity_summary_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"element_similarity_summary_Llama.csv\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def safe_text(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(x)\n",
    "\n",
    "def rougeL_recall(ref: str, gen: str) -> float:\n",
    "    ref = ref or \"\"\n",
    "    gen = gen or \"\"\n",
    "    if not ref.strip() and not gen.strip():\n",
    "        return 1.0\n",
    "    if not ref.strip() or not gen.strip():\n",
    "        return 0.0\n",
    "    return float(rouge.score(ref, gen)[\"rougeL\"].recall)\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "for model_name, in_path in INPUTS.items():\n",
    "    if not in_path.exists():\n",
    "        print(f\"⚠️ Missing input: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    required = {\"title\", \"Element number\", \"NIH Value\", \"Generated Content\"}\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{in_path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    # normalize\n",
    "    df[\"title\"] = df[\"title\"].apply(safe_text)\n",
    "    df[\"Element number\"] = df[\"Element number\"].apply(safe_text).str.strip().str.lower()\n",
    "    df[\"NIH Value\"] = df[\"NIH Value\"].apply(safe_text)\n",
    "    df[\"Generated Content\"] = df[\"Generated Content\"].apply(safe_text)\n",
    "\n",
    "    # ---- SBERT (batch encode) ----\n",
    "    nih_texts = df[\"NIH Value\"].tolist()\n",
    "    gen_texts = df[\"Generated Content\"].tolist()\n",
    "\n",
    "    # Encode in batches; normalize embeddings for stable cosine similarity\n",
    "    emb_nih = sbert.encode(nih_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "    emb_gen = sbert.encode(gen_texts, convert_to_tensor=True, normalize_embeddings=True)\n",
    "\n",
    "    # Cosine similarity per row (dot product works because normalized)\n",
    "    sbert_sims = (emb_nih * emb_gen).sum(dim=1).cpu().numpy().astype(float)\n",
    "\n",
    "    # ---- ROUGE-L recall (row-wise) ----\n",
    "    rouge_recalls = [\n",
    "        rougeL_recall(r, g) for r, g in zip(nih_texts, gen_texts)\n",
    "    ]\n",
    "\n",
    "    raw = pd.DataFrame({\n",
    "        \"Model\": model_name,\n",
    "        \"DMP Title\": df[\"title\"],\n",
    "        \"Element number\": df[\"Element number\"],\n",
    "        \"SBERT_Similarity\": sbert_sims,\n",
    "        \"ROUGE_L_Recall\": rouge_recalls,\n",
    "    })\n",
    "\n",
    "    # Folder (DMP)-level summary\n",
    "    folder_summary = (\n",
    "        raw.groupby([\"Model\", \"DMP Title\"], as_index=False)[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]]\n",
    "           .mean()\n",
    "           .rename(columns={\"DMP Title\": \"Folder\"})\n",
    "    )\n",
    "\n",
    "    # Element-level summary\n",
    "    element_summary = (\n",
    "        raw.groupby([\"Model\", \"Element number\"], as_index=False)[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]]\n",
    "           .mean()\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    raw.to_csv(OUT_RAW[model_name], index=False, encoding=\"utf-8\")\n",
    "    folder_summary.to_csv(OUT_FOLDER[model_name], index=False, encoding=\"utf-8\")\n",
    "    element_summary.to_csv(OUT_ELEMENT[model_name], index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(f\" Saved Step 8 outputs for {model_name}:\")\n",
    "    print(f\"   - {OUT_FOLDER[model_name].as_posix()}\")\n",
    "    print(f\"   - {OUT_RAW[model_name].as_posix()}\")\n",
    "    print(f\"   - {OUT_ELEMENT[model_name].as_posix()}\")\n",
    "\n",
    "print(\"Done: Step 8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SBERT_Similarity  ROUGE_L_Recall\n",
      "count        156.000000      156.000000\n",
      "mean           0.688753        0.266886\n",
      "std            0.126661        0.109872\n",
      "min            0.323550        0.080000\n",
      "25%            0.617297        0.191332\n",
      "50%            0.700317        0.244146\n",
      "75%            0.778203        0.325333\n",
      "max            0.931892        0.675000\n",
      "Zero Generated cases (approx): 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "gpt_raw = pd.read_csv(Path(\"derivative\") / \"element_similarity_raw_Gpt.csv\")\n",
    "print(gpt_raw[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]].describe())\n",
    "print(\"Zero Generated cases (approx):\", (gpt_raw[\"ROUGE_L_Recall\"] == 0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: derivative\\filtered_Gpt.csv | rows=234\n",
      "status\n",
      "ok    234\n",
      "Name: count, dtype: int64\n",
      "Saved: derivative\\filtered_Llama.csv | rows=312\n",
      "status\n",
      "ok    312\n",
      "Name: count, dtype: int64\n",
      "Done: Step 5 extraction completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "PRIMARY_DIR = Path(\"primary\")\n",
    "HUMAN_XLSX  = PRIMARY_DIR / \"Human.xlsx\"\n",
    "GPT_DIR     = PRIMARY_DIR / \"gpt-DMPs\"\n",
    "LLAMA_DIR   = PRIMARY_DIR / \"llama-DMPs\"\n",
    "\n",
    "DERIV_DIR = Path(\"derivative\")          # <-- top-level folder\n",
    "DERIV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GPT_OUT_CSV   = DERIV_DIR / \"filtered_Gpt.csv\"\n",
    "LLAMA_OUT_CSV = DERIV_DIR / \"filtered_Llama.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def norm_text(x) -> str:\n",
    "    \"\"\"Lowercase + collapse whitespace; safe for NaN.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def clean_title_for_match(name: str) -> str:\n",
    "    \"\"\"Normalize Excel titles for filename matching (keeps spaces).\"\"\"\n",
    "    s = str(name).strip()\n",
    "    s = re.sub(r'[\\\\/*?:\"<>|]', \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_md_stem(stem: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove trailing model suffixes:\n",
    "      \"...-gpt-4.1\" -> \"...\"\n",
    "      \"...-llama3.3\" -> \"...\"\n",
    "    \"\"\"\n",
    "    s = (stem or \"\").strip().lower()\n",
    "    s = re.sub(r\"[-_\\s]?gpt[-_\\s]?[\\d\\.]+$\", \"\", s)\n",
    "    s = re.sub(r\"[-_\\s]?llama[-_\\s]?[\\d\\.]+$\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def find_md_by_excel_title(search_dir: Path, excel_title: str) -> Path | None:\n",
    "    \"\"\"Find the .md file whose normalized stem matches the Excel title.\"\"\"\n",
    "    excel_norm = norm_text(clean_title_for_match(excel_title))\n",
    "    md_files = list(search_dir.rglob(\"*.md\"))\n",
    "\n",
    "    # Exact normalized match\n",
    "    for p in md_files:\n",
    "        if normalize_md_stem(p.stem) == excel_norm:\n",
    "            return p\n",
    "\n",
    "    # Fallback: contains match\n",
    "    candidates = [p for p in md_files if excel_norm and excel_norm in normalize_md_stem(p.stem)]\n",
    "    if candidates:\n",
    "        candidates = sorted(candidates, key=lambda x: (len(x.stem), str(x)))\n",
    "        return candidates[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "def is_title(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect NIH element headers in markdown.\n",
    "    Supports:\n",
    "      - Markdown headers: '#', '##', ...\n",
    "      - Numbered bold headings: '1. **Data Type**'\n",
    "    \"\"\"\n",
    "    s = (line or \"\").strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    if s.startswith(\"#\"):\n",
    "        return True\n",
    "    return bool(re.match(r\"^\\s*\\d+[\\.\\)]?\\s*\\*\\*.+\\*\\*\\s*$\", s))\n",
    "\n",
    "def extract_titles_and_text(md_text: str, content_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Extract titles and their content; remove <think>...</think>.\"\"\"\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", md_text, flags=re.DOTALL | re.IGNORECASE)\n",
    "    lines = cleaned.splitlines()\n",
    "\n",
    "    rows = []\n",
    "    current_title = None\n",
    "    buf = []\n",
    "\n",
    "    def flush():\n",
    "        nonlocal current_title, buf, rows\n",
    "        if current_title is None:\n",
    "            return\n",
    "        text = \"\\n\".join(buf).strip()\n",
    "        if text:\n",
    "            rows.append({\"Element title\": current_title.strip(), content_col: text})\n",
    "\n",
    "    for line in lines:\n",
    "        if is_title(line):\n",
    "            flush()\n",
    "            current_title = line\n",
    "            buf = []\n",
    "        else:\n",
    "            buf.append(line)\n",
    "\n",
    "    flush()\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def choose_title_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"title\", \"Title\", \"dmp_title\", \"DMP Title\", \"DMP_title\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"Could not find title column. Columns: {list(df.columns)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load titles from Human.xlsx\n",
    "# -----------------------------\n",
    "if not HUMAN_XLSX.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {HUMAN_XLSX}\")\n",
    "\n",
    "human_df = pd.read_excel(HUMAN_XLSX)\n",
    "title_col = choose_title_column(human_df)\n",
    "titles = human_df[title_col].dropna().astype(str).tolist()\n",
    "\n",
    "# -----------------------------\n",
    "# Process model folder\n",
    "# -----------------------------\n",
    "def process_model_folder(model_dir: Path, content_col: str, out_csv: Path) -> pd.DataFrame:\n",
    "    records = []\n",
    "\n",
    "    for title in titles:\n",
    "        md_path = find_md_by_excel_title(model_dir, title)\n",
    "\n",
    "        if md_path is None:\n",
    "            records.append({\n",
    "                \"dmp_title\": title,\n",
    "                \"md_path\": None,\n",
    "                \"Element title\": None,\n",
    "                content_col: None,\n",
    "                \"status\": \"missing_md\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        md_text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        df_sec = extract_titles_and_text(md_text, content_col=content_col)\n",
    "\n",
    "        if df_sec.empty:\n",
    "            records.append({\n",
    "                \"dmp_title\": title,\n",
    "                \"md_path\": str(md_path),\n",
    "                \"Element title\": None,\n",
    "                content_col: None,\n",
    "                \"status\": \"no_sections_found\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        for _, r in df_sec.iterrows():\n",
    "            records.append({\n",
    "                \"dmp_title\": title,\n",
    "                \"md_path\": str(md_path),\n",
    "                \"Element title\": r[\"Element title\"],\n",
    "                content_col: r[content_col],\n",
    "                \"status\": \"ok\"\n",
    "            })\n",
    "\n",
    "    out_df = pd.DataFrame(records)\n",
    "    out_df.to_csv(out_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_csv} | rows={len(out_df)}\")\n",
    "    print(out_df[\"status\"].value_counts(dropna=False))\n",
    "    return out_df\n",
    "\n",
    "# -----------------------------\n",
    "# Run Step 5\n",
    "# -----------------------------\n",
    "process_model_folder(GPT_DIR,   content_col=\"Generated_Gpt_content\",   out_csv=GPT_OUT_CSV)\n",
    "process_model_folder(LLAMA_DIR, content_col=\"Generated_Llama_content\", out_csv=LLAMA_OUT_CSV)\n",
    "\n",
    "print(\"Done: Step 5 extraction completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: derivative\\merged_output_Gpt.csv | rows=338\n",
      "Saved: derivative\\merged_output_Llama.csv | rows=338\n",
      "Done: Step 6 merge completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "PRIMARY_DIR = Path(\"primary\")\n",
    "HUMAN_XLSX  = PRIMARY_DIR / \"Human.xlsx\"\n",
    "\n",
    "DERIV_DIR = Path(\"derivative\")\n",
    "DERIV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GPT_FILTERED   = DERIV_DIR / \"filtered_Gpt.csv\"\n",
    "LLAMA_FILTERED = DERIV_DIR / \"filtered_Llama.csv\"\n",
    "\n",
    "GPT_MERGED   = DERIV_DIR / \"merged_output_Gpt.csv\"\n",
    "LLAMA_MERGED = DERIV_DIR / \"merged_output_Llama.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def norm_text(x) -> str:\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = str(x).strip().lower()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def choose_title_column(df: pd.DataFrame) -> str:\n",
    "    candidates = [\"title\", \"Title\", \"dmp_title\", \"DMP Title\", \"DMP_title\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"Could not find title column. Columns: {list(df.columns)}\")\n",
    "\n",
    "def get_element_columns(df: pd.DataFrame) -> list:\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        cc = str(c).strip().lower()\n",
    "        if re.match(r\"^element_\\d+[a-z]?$\", cc):\n",
    "            cols.append(cc)\n",
    "    if not cols:\n",
    "        raise ValueError(\n",
    "            \"No element_* columns found in Human.xlsx. \"\n",
    "            \"Expected: element_1a, element_1b, ..., element_6.\"\n",
    "        )\n",
    "\n",
    "    def sort_key(c):\n",
    "        m = re.match(r\"^element_(\\d+)([a-z]?)$\", c)\n",
    "        num = int(m.group(1))\n",
    "        suf = m.group(2) or \"\"\n",
    "        return (num, suf)\n",
    "\n",
    "    return sorted(cols, key=sort_key)\n",
    "\n",
    "# --- mapping markdown section title -> NIH element key ---\n",
    "def build_element_map():\n",
    "    return [\n",
    "        (re.compile(r\"\\b1\\b.*\\bdata\\b.*\\btype\\b\", re.I), \"element_1a\"),\n",
    "        (re.compile(r\"\\b1\\b.*\\bformat\\b\", re.I), \"element_1b\"),\n",
    "        (re.compile(r\"\\b1\\b.*\\bmetadata\\b|\\bdocumentation\\b\", re.I), \"element_1c\"),\n",
    "\n",
    "        (re.compile(r\"\\b2\\b.*\\btools\\b|\\bsoftware\\b|\\bcode\\b\", re.I), \"element_2\"),\n",
    "        (re.compile(r\"\\b3\\b.*\\bstandards\\b|\\bquality\\b|\\bcontrol\\b\", re.I), \"element_3\"),\n",
    "\n",
    "        (re.compile(r\"\\b4\\b.*\\brepository\\b|\\bpreserv\", re.I), \"element_4a\"),\n",
    "        (re.compile(r\"\\b4\\b.*\\btimeline\\b|\\bshare\\b|\\brelease\\b\", re.I), \"element_4b\"),\n",
    "        (re.compile(r\"\\b4\\b.*\\baccess\\b|\\brestriction\\b|\\bcontrolled\\b\", re.I), \"element_4c\"),\n",
    "\n",
    "        (re.compile(r\"\\b5\\b.*\\bprivacy\\b|\\bconfidential\", re.I), \"element_5a\"),\n",
    "        (re.compile(r\"\\b5\\b.*\\bsecurity\\b|\\bencrypt\\b|\\bstorage\\b\", re.I), \"element_5b\"),\n",
    "        (re.compile(r\"\\b5\\b.*\\bconsent\\b|\\birb\\b|\\birs\\b|\\bcompliance\\b\", re.I), \"element_5c\"),\n",
    "\n",
    "        (re.compile(r\"\\b6\\b.*\\boversight\\b|\\bresponsib\", re.I), \"element_6\"),\n",
    "    ]\n",
    "\n",
    "ELEMENT_MAP = build_element_map()\n",
    "\n",
    "def map_section_title_to_element(section_title) -> str | None:\n",
    "    if section_title is None:\n",
    "        return None\n",
    "    try:\n",
    "        if pd.isna(section_title):\n",
    "            return None\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    t = str(section_title)\n",
    "    t = re.sub(r\"^#+\\s*\", \"\", t).strip()\n",
    "\n",
    "    for pattern, element_key in ELEMENT_MAP:\n",
    "        if pattern.search(t):\n",
    "            return element_key\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Load NIH reference (Human.xlsx)\n",
    "# -----------------------------\n",
    "ref = pd.read_excel(HUMAN_XLSX)\n",
    "ref.columns = [str(c).strip() for c in ref.columns]\n",
    "\n",
    "title_col = choose_title_column(ref)\n",
    "ref = ref.rename(columns={title_col: \"title\"})\n",
    "ref.columns = [str(c).strip().lower() for c in ref.columns]\n",
    "\n",
    "ref[\"title_norm\"] = ref[\"title\"].apply(norm_text)\n",
    "element_cols = get_element_columns(ref)\n",
    "\n",
    "ref = ref[[\"title\", \"title_norm\"] + element_cols].fillna(\"\")\n",
    "\n",
    "ref_long = ref.melt(\n",
    "    id_vars=[\"title\", \"title_norm\"],\n",
    "    value_vars=element_cols,\n",
    "    var_name=\"Element number\",\n",
    "    value_name=\"NIH Value\",\n",
    ")\n",
    "ref_long[\"Element number\"] = ref_long[\"Element number\"].str.lower().str.strip()\n",
    "\n",
    "# -----------------------------\n",
    "# Load Step 5 CSVs + map section titles to element keys\n",
    "# -----------------------------\n",
    "def load_model_filtered(path: Path, content_col: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    needed = {\"dmp_title\", \"Element title\", content_col}\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    df[\"title_norm\"] = df[\"dmp_title\"].apply(norm_text)\n",
    "    df[\"Element title\"] = df[\"Element title\"].fillna(\"\").astype(str)\n",
    "\n",
    "    df[\"Element number\"] = df[\"Element title\"].apply(map_section_title_to_element)\n",
    "    df = df[df[\"Element number\"].notna()].copy()\n",
    "    df[\"Element number\"] = df[\"Element number\"].str.lower().str.strip()\n",
    "\n",
    "    df = df.rename(columns={content_col: \"Generated Content\"})\n",
    "    return df[[\"dmp_title\", \"title_norm\", \"Element number\", \"Element title\", \"Generated Content\"]]\n",
    "\n",
    "gpt_sec = load_model_filtered(GPT_FILTERED, content_col=\"Generated_Gpt_content\")\n",
    "llm_sec = load_model_filtered(LLAMA_FILTERED, content_col=\"Generated_Llama_content\")\n",
    "\n",
    "# -----------------------------\n",
    "# Merge + Save\n",
    "# -----------------------------\n",
    "def merge_and_save(ref_long: pd.DataFrame, sec_df: pd.DataFrame, out_path: Path):\n",
    "    merged = ref_long.merge(sec_df, on=[\"title_norm\", \"Element number\"], how=\"left\")\n",
    "    merged = merged[[\"title\", \"Element number\", \"NIH Value\", \"Element title\", \"Generated Content\"]]\n",
    "    merged.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Saved: {out_path} | rows={len(merged)}\")\n",
    "\n",
    "merge_and_save(ref_long, gpt_sec, GPT_MERGED)\n",
    "merge_and_save(ref_long, llm_sec, LLAMA_MERGED)\n",
    "\n",
    "print(\"Done: Step 6 merge completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved: derivative\\merged_output_Gpt_cleaned.csv\n",
      " Saved: derivative\\merged_output_Llama_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "DERIV_DIR = Path(\"derivative\")\n",
    "\n",
    "INPUTS = {\n",
    "    \"Gpt\": DERIV_DIR / \"merged_output_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"merged_output_Llama.csv\",\n",
    "}\n",
    "\n",
    "OUTPUTS = {\n",
    "    \"Gpt\": DERIV_DIR / \"merged_output_Gpt_cleaned.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"merged_output_Llama_cleaned.csv\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Groups to collapse\n",
    "# -----------------------------\n",
    "group_map = {\n",
    "    \"element_1\": [\"element_1a\", \"element_1b\", \"element_1c\"],\n",
    "    \"element_4\": [\"element_4a\", \"element_4b\", \"element_4c\"],\n",
    "    \"element_5\": [\"element_5a\", \"element_5b\", \"element_5c\"],\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def clean_text(text) -> str:\n",
    "    \"\"\"Remove multiple blank lines, leading/trailing spaces/newlines; safe for NaN.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    s = str(text).strip()\n",
    "    # Replace 2+ consecutive newlines (with optional whitespace) by a single blank line\n",
    "    s = re.sub(r\"\\n\\s*\\n+\", \"\\n\\n\", s)\n",
    "    return s\n",
    "\n",
    "def sort_element_key(x: str):\n",
    "    \"\"\"\n",
    "    Sort element numbers in a logical order:\n",
    "      element_1, element_2, element_3, element_4, element_5, element_6\n",
    "      and if any subelements remain, keep them after their parent.\n",
    "    \"\"\"\n",
    "    s = str(x).strip().lower()\n",
    "    m = re.match(r\"^element_(\\d+)([a-z]?)$\", s)\n",
    "    if not m:\n",
    "        return (999, \"z\", s)\n",
    "    num = int(m.group(1))\n",
    "    suf = m.group(2) or \"\"\n",
    "    return (num, suf, s)\n",
    "\n",
    "def combine_group(df: pd.DataFrame, new_element: str, group: list[str]) -> dict:\n",
    "    group_df = df[df[\"Element number\"].isin(group)].copy()\n",
    "\n",
    "    # Join NIH Value and Generated Content in the group order (1a,1b,1c etc.)\n",
    "    group_df[\"Element number\"] = pd.Categorical(group_df[\"Element number\"], categories=group, ordered=True)\n",
    "    group_df = group_df.sort_values(\"Element number\")\n",
    "\n",
    "    merged_value = \"\\n\\n\".join([clean_text(x) for x in group_df[\"NIH Value\"].tolist() if clean_text(x)])\n",
    "    merged_generated = \"\\n\\n\".join([clean_text(x) for x in group_df[\"Generated Content\"].tolist() if clean_text(x)])\n",
    "\n",
    "    return {\n",
    "        \"Element number\": new_element,\n",
    "        \"NIH Value\": merged_value,\n",
    "        \"Generated Content\": merged_generated,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "for model_name, in_path in INPUTS.items():\n",
    "    if not in_path.exists():\n",
    "        print(f\" Missing input file: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    # Basic validation\n",
    "    required_cols = {\"Element number\", \"NIH Value\", \"Generated Content\"}\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{in_path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    # Normalize element number casing\n",
    "    df[\"Element number\"] = df[\"Element number\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "    # Keep non-group rows\n",
    "    group_flat = [e for group in group_map.values() for e in group]\n",
    "    remaining_df = df[~df[\"Element number\"].isin(group_flat)].copy()\n",
    "\n",
    "    # Build collapsed rows\n",
    "    merged_rows = []\n",
    "    for new_element, group in group_map.items():\n",
    "        merged_rows.append(combine_group(df, new_element, group))\n",
    "\n",
    "    final_df = pd.concat([remaining_df, pd.DataFrame(merged_rows)], ignore_index=True)\n",
    "\n",
    "    # Clean text columns\n",
    "    for col in [\"NIH Value\", \"Generated Content\"]:\n",
    "        final_df[col] = final_df[col].apply(clean_text)\n",
    "\n",
    "    # Sort nicely\n",
    "    final_df = final_df.sort_values(by=\"Element number\", key=lambda s: s.map(sort_element_key)).reset_index(drop=True)\n",
    "\n",
    "    out_path = OUTPUTS[model_name]\n",
    "    final_df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\" Saved: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1587.34it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved for Gpt:\n",
      "   - derivative\\folder_similarity_summary_Gpt.csv\n",
      "   - derivative\\element_similarity_raw_Gpt.csv\n",
      "   - derivative\\element_similarity_summary_Gpt.csv\n",
      " Saved for Llama:\n",
      "   - derivative\\folder_similarity_summary_Llama.csv\n",
      "   - derivative\\element_similarity_raw_Llama.csv\n",
      "   - derivative\\element_similarity_summary_Llama.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "DERIV_DIR = Path(\"derivative\")\n",
    "\n",
    "# Use cleaned merged outputs (recommended)\n",
    "INPUTS = {\n",
    "    \"Gpt\":   DERIV_DIR / \"merged_output_Gpt_cleaned.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"merged_output_Llama_cleaned.csv\",\n",
    "}\n",
    "\n",
    "# Output files (saved in derivative/)\n",
    "OUT_FOLDER_SUMMARY = {\n",
    "    \"Gpt\":   DERIV_DIR / \"folder_similarity_summary_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"folder_similarity_summary_Llama.csv\",\n",
    "}\n",
    "OUT_ELEMENT_RAW = {\n",
    "    \"Gpt\":   DERIV_DIR / \"element_similarity_raw_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"element_similarity_raw_Llama.csv\",\n",
    "}\n",
    "OUT_ELEMENT_SUMMARY = {\n",
    "    \"Gpt\":   DERIV_DIR / \"element_similarity_summary_Gpt.csv\",\n",
    "    \"Llama\": DERIV_DIR / \"element_similarity_summary_Llama.csv\",\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def safe_text(x) -> str:\n",
    "    \"\"\"Convert to string; return '' for NaN/None.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    return str(x)\n",
    "\n",
    "def compute_sbert_similarity(t1: str, t2: str) -> float:\n",
    "    if not t1.strip() and not t2.strip():\n",
    "        return 1.0  # both empty => perfect match (you can change to 0.0 if you prefer)\n",
    "    if not t1.strip() or not t2.strip():\n",
    "        return 0.0\n",
    "    emb1 = sbert.encode(t1, convert_to_tensor=True)\n",
    "    emb2 = sbert.encode(t2, convert_to_tensor=True)\n",
    "    return float(util.cos_sim(emb1, emb2).item())\n",
    "\n",
    "def compute_rougeL_recall(t_ref: str, t_gen: str) -> float:\n",
    "    if not t_ref.strip() and not t_gen.strip():\n",
    "        return 1.0\n",
    "    if not t_ref.strip() or not t_gen.strip():\n",
    "        return 0.0\n",
    "    scores = rouge.score(t_ref, t_gen)\n",
    "    return float(scores[\"rougeL\"].recall)\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop per model\n",
    "# -----------------------------\n",
    "for model_name, in_path in INPUTS.items():\n",
    "    if not in_path.exists():\n",
    "        print(f\" Missing input: {in_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(in_path)\n",
    "\n",
    "    # Expected columns from Step 6:\n",
    "    # title, Element number, NIH Value, Element title, Generated Content\n",
    "    required_cols = [\"title\", \"Element number\", \"NIH Value\", \"Generated Content\"]\n",
    "    missing = [c for c in required_cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{in_path} missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    element_rows = []\n",
    "\n",
    "    # Compute per-row scores\n",
    "    for _, row in df.iterrows():\n",
    "        dmp_title = safe_text(row[\"title\"])\n",
    "        elem = safe_text(row[\"Element number\"]).strip().lower()\n",
    "\n",
    "        nih_text = safe_text(row[\"NIH Value\"])\n",
    "        gen_text = safe_text(row[\"Generated Content\"])\n",
    "\n",
    "        sbert_sim = compute_sbert_similarity(nih_text, gen_text)\n",
    "        rouge_recall = compute_rougeL_recall(nih_text, gen_text)\n",
    "\n",
    "        element_rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"DMP Title\": dmp_title,\n",
    "            \"Element number\": elem,\n",
    "            \"SBERT_Similarity\": sbert_sim,\n",
    "            \"ROUGE_L_Recall\": rouge_recall,\n",
    "        })\n",
    "\n",
    "    element_raw = pd.DataFrame(element_rows)\n",
    "\n",
    "    # Folder (DMP)-level summary: mean across elements for each DMP\n",
    "    folder_summary = (\n",
    "        element_raw\n",
    "        .groupby([\"Model\", \"DMP Title\"], as_index=False)[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]]\n",
    "        .mean()\n",
    "        .rename(columns={\"DMP Title\": \"Folder\"})\n",
    "    )\n",
    "\n",
    "    # Element-level summary: mean across DMPs for each element\n",
    "    element_summary = (\n",
    "        element_raw\n",
    "        .groupby([\"Model\", \"Element number\"], as_index=False)[[\"SBERT_Similarity\", \"ROUGE_L_Recall\"]]\n",
    "        .mean()\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    element_raw.to_csv(OUT_ELEMENT_RAW[model_name], index=False)\n",
    "    folder_summary.to_csv(OUT_FOLDER_SUMMARY[model_name], index=False)\n",
    "    element_summary.to_csv(OUT_ELEMENT_SUMMARY[model_name], index=False)\n",
    "\n",
    "    print(f\" Saved for {model_name}:\")\n",
    "    print(f\"   - {OUT_FOLDER_SUMMARY[model_name]}\")\n",
    "    print(f\"   - {OUT_ELEMENT_RAW[model_name]}\")\n",
    "    print(f\"   - {OUT_ELEMENT_SUMMARY[model_name]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
