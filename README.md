# Code: Automated and Human Evaluation Generated NIH Data Management Plans (DMPs)

## Overview

This repository contains the code associated with the paper, “Evaluating the Performance of LLMs in Drafting NIH Data Management Plans.” The project evaluates NIH Data Management Plans using two complementary approaches: automatic reference-based evaluation and human expert evaluation.

The repository includes the complete automated and human evaluation workflows, along with analysis results for DMPs generated by Llama 3.3 and GPT-4.1, as well as a set of human-written DMPs selected for this study. Please refer to the project **[inventory](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-inventory/tree/main)** for all related resources, including the paper.

------------------------------------------------------------------------
## Standards followed
The overall codebase is organized in alignment with the **[FAIR-BioRS guidelines](https://fair-biors.org/)**. All Python code follows **[PEP 8](https://peps.python.org/pep-0008/)** conventions, including consistent formatting, inline comments, and docstrings. Project dependencies are fully captured in **[requirements.txt](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/Requirements.txt)**. 

------------------------------------------------------------------------


## Getting Started
### Step 1 — Clone the repository
```bash
git clone https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code.git
cd dmpchef
code .
```
### Step 2 — Create and activate a virtual environment

**Windows (cmd):**
```bash
python -m venv venv
venv\Scripts\activate.bat
```
**macOS/Linux:**
```bash
python -m venv venv
source venv/bin/activate
```
### Step 3 — Install dependencies
```bash
pip install -r requirements.txt
```
---

### step 4- Running the Notebook in two approaches 
This repository supports two complementary evaluation workflows. Use the appropriate notebook depending on the evaluation approach you want to run.

#### Automatic reference-based evaluation

Use: [`Automated-evaluation.ipynb`](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/Automated-evaluation.ipynb)

#### Human expert evaluation

Use: [`Human-evaluation.ipynb`](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/Human-evaluation.ipynb)


------------------------------------------------------------------------

## Inputs and Outputs

The Jupyter notebook makes use of files in the dataset associated with the paper. You will need to download the dataset at add it in the input folder (call the dataset folder 'dataset').

All outputs from both evaluation pipelines (tables and figures) are saved under the `outputs/` directory.

------------------------------------------------------------------------

## License
This work is licensed under the **[MIT License](https://opensource.org/license/mit/)**. See **[LICENSE](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/LICENSE)** for more information.

------------------------------------------------------------------------

## Feedback and contribution
Use **[GitHub Issues](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/issues)** to submit feedback, report problems, or suggest improvements.  
You can also **fork** the repository and submit a **Pull Request** with your changes.

------------------------------------------------------------------------

## How to cite
If you use this code, please cite this repository using the **versioned DOI on Zenodo** for the specific release you used (instructions will be added once the Zenodo record is available). For now, you can reference the repository here: [`fairdataihub/nih-dmp-llm-generation`](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code). 
