# Code: Automated and Human Evaluation Generated NIH Data Management Plans (DMPs)

## Overview

This repository contains the code associated with the paper, “Evaluating the Performance of LLMs in Drafting NIH Data Management Plans.” The project evaluates DMPs using two complementary approaches: automatic reference-based evaluation and human expert evaluation. The repository includes the evaluation workflow and results for DMPs generated by Llama 3.3 and GPT-4.1, along with a set of human-written DMPs selected for the purposes of this study. 


------------------------------------------------------------------------
## Standards followed
The overall codebase is organized in alignment with the **[FAIR-BioRS guidelines](https://fair-biors.org/)**. All Python code follows **[PEP 8](https://peps.python.org/pep-0008/)** conventions, including consistent formatting, inline comments, and docstrings. Project dependencies are fully captured in **[requirements.txt](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/Requirements.txt)**. 

------------------------------------------------------------------------


## Getting Started
### Step 1 — Clone the repository
```bash
git clone https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code.git
cd dmpchef
code .
```
### Step 2 — Create and activate a virtual environment

**Windows (cmd):**
```bash
python -m venv venv
venv\Scripts\activate.bat
```
**macOS/Linux:**
```bash
python -m venv venv
source venv/bin/activate
```
### Step 3 — Install dependencies
```bash
pip install -r requirements.txt
# or (recommended for local dev)
pip install -e .
```
---

### step 4- Running the Notebook in two approaches 
**automatic reference-based evaluation**
Use **[`Automated-evaluation.ipynb`](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/Automated-evaluation.ipynb)**.
**human expert evaluation**
Use **[`Human-evaluation.ipynb`](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/Human-evaluation.ipynb)**.

------------------------------------------------------------------------

## Inputs and Outputs

Both notebooks use structured input files included in the `primary/` directory.

- The automatic reference-based evaluation inputs include:
  - Llama 3.3 generated DMPs (Markdown)
  - GPT-4.1 generated DMPs (Markdown)
  - Extracted content from selected human-written DMPs (Excel)

- The human expert evaluation inputs and survey-derived tables and figures are also organized under `primary/`.

The DMP template used for generation and evaluation is available here: [`primary/automated-evaluation/dmp-template.md`](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/primary/automated-evaluation/dmp-template.md)

All outputs from both evaluation pipelines (tables and figures) are saved under the `derivative/` directory: https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/tree/main/derivative


------------------------------------------------------------------------

## License
This work is licensed under the **[MIT License](https://opensource.org/license/mit/)**. See **[LICENSE](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/blob/main/LICENSE)** for more information.

------------------------------------------------------------------------

## Feedback and contribution
Use **[GitHub Issues](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code/issues)** to submit feedback, report problems, or suggest improvements.  
You can also **fork** the repository and submit a **Pull Request** with your changes.

------------------------------------------------------------------------

## How to cite
If you use this code, please cite this repository using the **versioned DOI on Zenodo** for the specific release you used (instructions will be added once the Zenodo record is available). For now, you can reference the repository here: **[fairdataihub/nih-dmp-llm-generation](https://github.com/fairdataihub/nih-dmp-llm-evaluation-paper-code)**.